{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8837156-6138-4868-beed-9f2fb9fb35f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import Synth90kDataset, synth90k_collate_fn\n",
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from models.crnn import CRNN, count_parameters\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm \n",
    "from models.ctc_decoder import ctc_decoder\n",
    "\n",
    "# Set multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "\n",
    "dataset_path = './data/mnt/ramdisk/max/90kDICT32px/'\n",
    "modes = ['train', 'val', 'test']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6532f066-a997-453c-bf58-67bf2d9091d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_all_batches(batch, device):\n",
    "\n",
    "    images, targets, target_lengths = batch['images'], \\\n",
    "                                      batch['targets'], \\\n",
    "                                    batch['target_lengths']\n",
    "    images, targets, target_lengths = images.to(device), \\\n",
    "                                       targets.to(device), \\\n",
    "                                      target_lengths.to(device)\n",
    "    return images, targets, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ce54f9-ca69-441a-886e-6b45a74fbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(preds, preds_length, targets, target_lengths, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    batch_size = images.size(0)\n",
    "    \n",
    "    loss = criterion(preds, targets, preds_length, target_lengths)\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(crnn.parameters(), 5) # gradient clipping with 5\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a34fbba-2694-4bdf-870a-5bde85a958b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(output, output_lengths, targets, target_lengths, \n",
    "                      decode_method = 'beam_search', beam_size = 10):\n",
    "    output_detach = output.detach()\n",
    "    preds = ctc_decoder(output_detach, method = decode_method, beam_size = beam_size)\n",
    "    \n",
    "    reals = targets.cpu().numpy().tolist()\n",
    "\n",
    "    \n",
    "    target_lengths = target_lengths.cpu().numpy().tolist()\n",
    "    \n",
    "    num_correct = 0\n",
    "    target_length_counter = 0\n",
    "    for pred, target_length in zip(preds, target_lengths):\n",
    "        real = reals[target_length_counter: target_length_counter + target_length]\n",
    "        target_length_counter += target_length\n",
    "\n",
    "        # print(pred, real)\n",
    "        if pred == real:\n",
    "            num_correct += 1\n",
    "\n",
    "    return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de3924f-b53a-412d-8e75-04a4fbe1c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7224612"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591e2221-388b-4841-ae89-14b32a9fadd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802734"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f1f96a-9fbe-4676-9027-62c220145ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset with mode: train\n",
      "Loading Dataset with mode: val\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace \n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_args = Namespace(\n",
    "    data_dir = './data/mnt/ramdisk/max/90kDICT32px/',\n",
    "    train_batch_size = 32,\n",
    "    eval_batch_size = 512,\n",
    "    epochs = 1000,\n",
    "    save_interval = 20,\n",
    "    cpu_workers = 8,\n",
    "    learning_rate = 0.05,\n",
    "    reload_checkpoint = None,\n",
    "    decode_method = 'beam_search',\n",
    "    beam_size = 10,\n",
    "    checkpoints_dir = 'checkpoints/',\n",
    "    img_width = 100,\n",
    "    img_height= 32,\n",
    "    map_to_seq = 64,\n",
    "    lstm_hidden = 256,\n",
    "    leaky_relu = False\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = Synth90kDataset(dataset_path, mode = 'train', \n",
    "                                img_height = train_args.img_height,\n",
    "                                img_width = train_args.img_width)\n",
    "valid_dataset = Synth90kDataset(dataset_path, mode = 'val', \n",
    "                                img_height = train_args.img_height,\n",
    "                                img_width = train_args.img_width)\n",
    "# test_dataset = Synth90kDataset(dataset_path, mode = 'test', \n",
    "#                                 img_height = train_args.img_height,\n",
    "#                                 img_width = train_args.img_width)\n",
    "\n",
    "reduced_train = len(train_dataset) // 100\n",
    "reduced_indices = torch.randperm(len(train_dataset))[:reduced_train]\n",
    "train_dataset_reduced = torch.utils.data.Subset(train_dataset, reduced_indices)\n",
    "\n",
    "reduced_val = len(valid_dataset) // 100\n",
    "reduced_indices = torch.randperm(len(train_dataset))[:reduced_val]\n",
    "val_dataset_reduced = torch.utils.data.Subset(valid_dataset, reduced_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset_reduced, batch_size = train_args.train_batch_size,\n",
    "                         shuffle = True, num_workers = train_args.cpu_workers,\n",
    "                        collate_fn = synth90k_collate_fn)\n",
    "valid_loader = DataLoader(val_dataset_reduced, batch_size = train_args.eval_batch_size,\n",
    "                         shuffle = True, num_workers = train_args.cpu_workers,\n",
    "                        collate_fn = synth90k_collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = train_args.eval_batch_size,\n",
    "#                          shuffle = True, num_workers = train_args.cpu_workers,\n",
    "#                         collate_fn = synth90k_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "545981a6-b608-4218-8dbb-14b8e274e286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72246"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5b99755-3b3d-428a-9a5d-fbd3f9554938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in this model are: 7839077\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(Synth90kDataset.LABEL2CHAR) + 1\n",
    "crnn = CRNN(1, train_args.img_height, train_args.img_width, \n",
    "            num_classes = num_classes,\n",
    "            leaky_relu = train_args.leaky_relu, \n",
    "            map_to_seq = train_args.map_to_seq,\n",
    "            lstm_hidden = train_args.lstm_hidden).to(device)\n",
    "print(f\"The number of parameters in this model are: {count_parameters(crnn)}\")\n",
    "if train_args.reload_checkpoint:\n",
    "    crnn.load_state_dict(torch.load(reload_checkpoint, map_location=device))\n",
    "\n",
    "optimizer = optim.Adadelta(crnn.parameters(), lr = train_args.learning_rate, rho = 0.9)\n",
    "criterion = nn.CTCLoss(reduction = 'sum',  zero_infinity = True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57482a9c-c2b1-4a58-bf3b-e985daaf2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7338f8d57e4a95baa91685a4853d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8eb297ae5f42a1b381e27bdcc8ccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "\u001b[1;34mEpoch 1/1000\u001b[0m\n",
      "Train Loss: 119.27\n",
      "Train Acc: 0.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 2/1000\u001b[0m\n",
      "Train Loss: 89.27\n",
      "Train Acc: 0.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 3/1000\u001b[0m\n",
      "Train Loss: 62.16\n",
      "Train Acc: 0.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 4/1000\u001b[0m\n",
      "Train Loss: 43.84\n",
      "Train Acc: 0.00%\n",
      "========================================\n",
      "\u001b[1;34mEpoch 5/1000\u001b[0m\n",
      "Train Loss: 32.98\n",
      "Train Acc: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m total_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     22\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     26\u001b[0m     images, targets, target_lengths \u001b[38;5;241m=\u001b[39m getting_all_batches(batch, device)\n\u001b[1;32m     27\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:307\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 307\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:502\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    500\u001b[0m     c \u001b[38;5;241m=\u001b[39m PipeClient(address)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mSocketClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(authkey, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthkey should be a byte string\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:630\u001b[0m, in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m socket\u001b[38;5;241m.\u001b[39msocket( \u001b[38;5;28mgetattr\u001b[39m(socket, family) ) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[1;32m    629\u001b[0m     s\u001b[38;5;241m.\u001b[39msetblocking(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m     \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Connection(s\u001b[38;5;241m.\u001b[39mdetach())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = train_args.epochs\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "epoch_bar = tqdm(desc = 'Epoch',\n",
    "                 total = num_epochs, position = 1)\n",
    "train_bar = tqdm(desc = 'Training', total = len(train_loader),\n",
    "                 position = 1, leave = True)\n",
    "# val_bar = tqdm(desc = 'Validation', total = len(test_loader),\n",
    "#                position = 1, leave = True)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_bar.set_description(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    crnn.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        images, targets, target_lengths = getting_all_batches(batch, device)\n",
    "        batch_size = batch['images'].size(0)\n",
    "\n",
    "        # print(images)\n",
    "        \n",
    "        preds = crnn(images)\n",
    "        preds = preds.permute(1, 0, 2) #(seq_len, batch, num_classes)\n",
    "        seq_length = preds.size(0)\n",
    "\n",
    "        # print(preds)\n",
    "        preds_lengths = torch.full(size = (batch_size, ), \n",
    "                                   fill_value = seq_length, \n",
    "                                   dtype = torch.long).to(device)\n",
    "        # print(preds_lengths)\n",
    "        # print(preds.shape)\n",
    "        # print(preds_lengths.shape)\n",
    "        # print(targets.shape)\n",
    "        # print(target_lengths.shape)\n",
    "        # print(torch.sum(target_lengths))\n",
    "        loss_t = calculate_loss(preds, preds_lengths, targets, target_lengths,\n",
    "                               optimizer , criterion)\n",
    "        \n",
    "\n",
    "        running_loss += (loss_t - running_loss) / (i + 1)\n",
    "        total_loss += loss_t \n",
    "        total += batch_size \n",
    "\n",
    "        \n",
    "        num_correct = calculate_accuracy(preds, preds_lengths, \n",
    "                                          targets, target_lengths, \n",
    "                                         decode_method = train_args.decode_method,\n",
    "                                         beam_size = train_args.beam_size)\n",
    "        acc_t = num_correct / batch_size * 100\n",
    "        running_acc += (acc_t - running_acc) / (i + 1)\n",
    "        total_acc += num_correct \n",
    "        \n",
    "        train_bar.set_postfix(loss = running_loss,\n",
    "                              acc = f\"{running_acc:.2f}%\",\n",
    "                              epoch = epoch + 1)\n",
    "        train_bar.update()\n",
    "    \n",
    "    current_loss = total_loss / len(train_loader)\n",
    "    current_acc = total_acc / total * 100\n",
    "    train_loss.append(current_loss)\n",
    "    train_acc.append(current_acc)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print(\"\\033[1;34m\" + f\"Epoch {epoch + 1}/{num_epochs}\" + \"\\033[0m\")\n",
    "    print(f\"Train Loss: {current_loss:.2f}\\nTrain Acc: {current_acc:.2f}%\")\n",
    "\n",
    "    train_bar.n = 0\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4507ce1-f4d8-4a39-9b42-45f0d9e6d267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
